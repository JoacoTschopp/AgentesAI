{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting whisper\n",
      "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in ./.venv/lib/python3.13/site-packages (from whisper) (1.17.0)\n",
      "Building wheels for collected packages: whisper\n",
      "  Building wheel for whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41197 sha256=dd0d15d03f463f2288aeaa8b717c5dd2a8345e9c30f100d90494356294538638\n",
      "  Stored in directory: /Users/joaquintschopp/Library/Caches/pip/wheels/7e/1e/f0/d36b92489c74925c5aa1aeb01d30f39ba018d2a1914e79ac36\n",
      "Successfully built whisper\n",
      "Installing collected packages: whisper\n",
      "Successfully installed whisper-1.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faster_whisper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaster_whisper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WhisperModel\n\u001b[32m      3\u001b[39m model = WhisperModel(\u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m, compute_type=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m segments, _ = model.transcribe(\u001b[33m\"\u001b[39m\u001b[33mAudios/INESPERADO.mp3\u001b[39m\u001b[33m\"\u001b[39m, word_timestamps=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faster_whisper'"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\"medium\", compute_type=\"float32\")\n",
    "segments, _ = model.transcribe(\"Audios/INESPERADO.mp3\", word_timestamps=True)\n",
    "\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f} - {segment.end:.2f}] {segment.text}\")\n",
    "    for word in segment.words:\n",
    "        print(f\"  {word.word} ({word.start:.2f}s - {word.end:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Y si en ese de abso que es bastante prolongado, no se terminó el juicio se cae el juicio. Lo hubiera obvido a la persona. Lo pusieron presos de alguna vez. Hace muchísimos años. ¿Por qué fue? Por decir la verdad, el juicio era, terminé su absuerto y el denunciado condenado. ¿Qué el juicio fue? Puedo vinculado a un tema de una prueba en una causa. Esto a mí me diberaron al otro condenado. Pero bueno. ¿Y quién lo puso preso? ¿Qué juez? O fervide, no sé si se cuarta. O termide, el juicio. El juicio se cuarta. El juicio se cuarta. El juicio hace unos años. Aquí nuestro cámara tenía una expresión. Y ministra, ver si.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"Audios/INESPERADO.mp3\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=\"/opt/homebrew/bin:$PATH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "libavutil      59. 39.100 / 59. 39.100\n",
      "libavcodec     61. 19.101 / 61. 19.101\n",
      "libavformat    61.  7.100 / 61.  7.100\n",
      "libavdevice    61.  3.100 / 61.  3.100\n",
      "libavfilter    10.  4.100 / 10.  4.100\n",
      "libswscale      8.  3.100 /  8.  3.100\n",
      "libswresample   5.  3.100 /  5.  3.100\n",
      "libpostproc    58.  3.100 / 58.  3.100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print(subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True).stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://sg-openai.openai.azure.com/ to https://sg-openai.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/langchain_community/chat_models/azure_openai.py:181: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/langchain_community/chat_models/azure_openai.py:189: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://sg-openai.openai.azure.com/ to https://sg-openai.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripción:\n",
      "Voz1: Y si en ese lapso, que es bastante prolongado, no se terminó el juicio, se cae el juicio. Los voy a obligar a hacerlo.\n",
      "Voz2: ¿Lo pusieron presa a usted alguna vez?\n",
      "Voz1: Hace muchísimos años.\n",
      "Voz2: ¿Por qué fue?\n",
      "Voz1: Por decir la verdad, en el juicio de Oramia, terminé ya absuelto y el denunciado condenado.\n",
      "Voz2: ¿Qué juicio fue?\n",
      "Voz1: Fue vinculado a un tema de una prueba en una causa. A mí me liberaron, al otro lo condenaron. Pero bueno.\n",
      "Voz2: ¿Y quién lo puso preso? ¿Qué juez?\n",
      "Voz1: Oyerbide, no sé si se acuerdan.\n",
      "Voz2: Oyerbide, el juez.\n",
      "Voz1: Yo lo conocí.\n",
      "Voz2: Sí, sí. El juez Oyerbide, sí.\n",
      "Voz1: Yo lo conocí. Ya hace unos años.\n",
      "Voz2: Aquí en nuestro cámara tenemos una expresión.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# 1. Transcripción de audio\n",
    "AUDIO_PATH = \"Audios/INESPERADO.mp3\"\n",
    "model = whisper.load_model(\"medium\")  # O \"base\" o \"large\"\n",
    "result = model.transcribe(AUDIO_PATH)\n",
    "\n",
    "transcription = result[\"text\"]\n",
    "\n",
    "# 2. Configurar AzureChatOpenAI\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Recuperar las variables\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "# Inicializar el LLM usando las variables de entorno\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=deployment_name,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Prompt de procesamiento (por ejemplo, identificar hablantes)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"texto\"],\n",
    "    template=\"\"\"\n",
    "A continuación hay una transcripción de un audio. Separá las partes donde habla una persona distinta y marcá con 'Voz1', 'Voz2', etc.\n",
    "\n",
    "Transcripción:\n",
    "{texto}\n",
    "\n",
    "---\n",
    "Resultado con voces identificadas:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "respuesta = chain.run(texto=transcription)\n",
    "\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Y si en ese lapso, que es bastante prolongado, no se terminó el juicio, se cae el juicio. Los voy a obligar a hacerlo. ¿Lo pusieron presa a usted alguna vez? Hace muchísimos años. ¿Por qué fue? Por decir la verdad, en el juicio de Oramia, terminé ya absuelto y el denunciado condenado. ¿Qué juicio fue? Fue vinculado a un tema de una prueba en una causa. A mí me liberaron, al otro lo condenaron. Pero bueno. ¿Y quién lo puso preso? ¿Qué juez? Oyerbide, no sé si se acuerdan. Oyerbide, el juez. Yo lo conocí. Sí, sí. El juez Oyerbide, sí. Yo lo conocí. Ya hace unos años. Aquí en nuestro cámara tenemos una expresión.\n"
     ]
    }
   ],
   "source": [
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00 - 6.36]  Y si en ese lapso, que es bastante prolongado, no se terminó el juicio, se cae el juicio.\n",
      "[6.36 - 8.04]  Los voy a obligar a hacerlo.\n",
      "[8.04 - 9.64]  ¿Lo pusieron presa a usted alguna vez?\n",
      "[9.64 - 11.04]  Hace muchísimos años.\n",
      "[11.04 - 12.04]  ¿Por qué fue?\n",
      "[12.04 - 19.24]  Por decir la verdad, en el juicio de Oramia, terminé ya absuelto y el denunciado condenado.\n",
      "[19.24 - 20.96]  ¿Qué juicio fue?\n",
      "[20.96 - 24.76]  Fue vinculado a un tema de una prueba en una causa.\n",
      "[24.76 - 28.24]  A mí me liberaron, al otro lo condenaron.\n",
      "[28.24 - 28.84]  Pero bueno.\n",
      "[28.84 - 29.84]  ¿Y quién lo puso preso?\n",
      "[29.88 - 30.88]  ¿Qué juez?\n",
      "[30.88 - 31.88]  Oyerbide, no sé si se acuerdan.\n",
      "[31.88 - 32.88]  Oyerbide, el juez.\n",
      "[32.88 - 33.88]  Yo lo conocí.\n",
      "[33.88 - 34.88]  Sí, sí.\n",
      "[34.88 - 35.88]  El juez Oyerbide, sí.\n",
      "[35.88 - 36.88]  Yo lo conocí.\n",
      "[36.88 - 37.88]  Ya hace unos años.\n",
      "[37.88 - 39.48]  Aquí en nuestro cámara tenemos una expresión.\n"
     ]
    }
   ],
   "source": [
    "for segment in result['segments']:\n",
    "    print(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
