{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting tinydb\n",
      "  Downloading tinydb-4.8.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Downloading tinydb-4.8.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tinydb\n",
      "Successfully installed tinydb-4.8.2\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain)\n",
      "  Using cached langchain_core-0.3.47-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.18-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.39-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.15-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.27.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langchain_core-0.3.47-py3-none-any.whl (417 kB)\n",
      "Using cached langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.18-py3-none-any.whl (351 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sqlalchemy-2.0.39-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl (195 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.10.15-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (633 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-extensions, tenacity, sniffio, PyYAML, orjson, jsonpointer, idna, h11, charset-normalizer, certifi, annotated-types, SQLAlchemy, requests, pydantic-core, jsonpatch, httpcore, anyio, requests-toolbelt, pydantic, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.39 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 charset-normalizer-3.4.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.21 langchain-core-0.3.47 langchain-text-splitters-0.3.7 langsmith-0.3.18 orjson-3.10.15 pydantic-2.10.6 pydantic-core-2.27.2 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 typing-extensions-4.12.2 urllib3-2.3.0 zstandard-0.23.0\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in ./.venv/lib/python3.13/site-packages (from langchain-community) (0.3.47)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in ./.venv/lib/python3.13/site-packages (from langchain-community) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.13/site-packages (from langchain-community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.13/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Using cached aiohttp-3.11.14-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.13/site-packages (from langchain-community) (0.3.18)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy<3,>=1.26.2 (from langchain-community)\n",
      "  Using cached numpy-2.2.4-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached multidict-6.2.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached propcache-0.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in ./.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.11.14-cp313-cp313-macosx_11_0_arm64.whl (453 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached numpy-2.2.4-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl (50 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.2.0-cp313-cp313-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached propcache-0.3.0-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl (91 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, propcache, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.5.0 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 multidict-6.2.0 mypy-extensions-1.0.0 numpy-2.2.4 propcache-0.3.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0 yarl-1.18.3\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install PyPDF2\n",
    "#!pip install tinydb\n",
    "#!pip install langchain\n",
    "#!pip install langchain-community\n",
    "#!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://sg-openai.openai.azure.com/ to https://sg-openai.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/langchain_community/chat_models/azure_openai.py:181: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/Users/joaquintschopp/ProyectosPY/AgentesAI/.venv/lib/python3.13/site-packages/langchain_community/chat_models/azure_openai.py:189: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://sg-openai.openai.azure.com/ to https://sg-openai.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "import os\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import PyPDF2\n",
    "from tinydb import TinyDB, Query\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Configuración del LLM (ajusta el parámetro 'temperature' o agrega tus credenciales si es necesario)\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=\"https://sg-openai.openai.azure.com/\",\n",
    "    openai_api_key=\"647e7eaa5678417f8667203df58216ee\",  # Reemplaza con tu clave de API\n",
    "    openai_api_version=\"2023-03-15-preview\",  # Ajusta según la versión que estés usando\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Inicialización de TinyDB (la base se almacenará en el archivo history.json)\n",
    "db = TinyDB(\"ResCTM.json\")\n",
    "records_table = db.table(\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def pdf_to_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrae el texto de un archivo PDF utilizando PyPDF2.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Corrección de la transcripción usando LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "correction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Corrige la transcripción del siguiente texto:\\n\\n{text}\\n\\nTexto corregido:\"\n",
    ")\n",
    "\n",
    "def correct_transcription(text):\n",
    "    \"\"\"\n",
    "    Utiliza un LLM para corregir la transcripción extraída del PDF.\n",
    "    \"\"\"\n",
    "    chain = LLMChain(llm=llm, prompt=correction_prompt)\n",
    "    corrected = chain.run(text=text)\n",
    "    return corrected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracción de datos y generación del resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt adaptado con instrucciones específicas\n",
    "extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"template_info\"],\n",
    "    template=(\n",
    "        \"Utilizando el siguiente template:\\n\"\n",
    "        \"{template_info}\\n\\n\"\n",
    "        \"Extrae únicamente los datos solicitados y genera un resumen ejecutivo breve (máximo 30 palabras) \"\n",
    "        \"del tema principal del texto, sin incluir detalles menores ni secundarios.\\n\\n\"\n",
    "        \"Texto:\\n{text}\\n\\n\"\n",
    "        \"Datos extraídos y resumen ejecutivo breve:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def extract_data_summary(text, extraction_template):\n",
    "    \"\"\"\n",
    "    Extrae datos específicos y genera un resumen ejecutivo breve.\n",
    "    \"\"\"\n",
    "    chain = LLMChain(llm=llm, prompt=extraction_prompt)\n",
    "    result = chain.run(text=text, template_info=extraction_template)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clasificación de la resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIAS_CTMSG = {\n",
    "    \"Reglamentación y Normativas\": [\n",
    "        \"REGLAMENTO\",\n",
    "        \"MANUAL DE NORMA\",\n",
    "        \"MANUAL Normas Compras y Contrataciones\",\n",
    "        \"NORMA ISO\",\n",
    "        \"LICENCIA ESPECIAL\",\n",
    "        \"ESTATUTO DEL PERSONAL\",\n",
    "        \"SUMARIO ADMINISTRATIVO\"\n",
    "    ],\n",
    "    \"Administración Financiera y Contable\": [\n",
    "        \"REMUNERACIONES\",\n",
    "        \"REGISTRACION CONTABLE\",\n",
    "        \"BALANCES\",\n",
    "        \"BANCOS\",\n",
    "        \"PRESUPUESTOS\",\n",
    "        \"TARIFAS ENERGÍA\"\n",
    "    ],\n",
    "    \"Contrataciones y Convenios\": [\n",
    "        \"CONVENIO\",\n",
    "        \"CONTRATACIONES Y COMPRAS\",\n",
    "        \"COMPRAS AUTOMOTORES\",\n",
    "        \"CONCURSOS PRECIOS\",\n",
    "        \"PODERES ABOGADOS\"\n",
    "    ],\n",
    "    \"Infraestructura y Mantenimiento\": [\n",
    "        \"EDIFICIOS\",\n",
    "        \"VIVIENDAS\",\n",
    "        \"EQUIPAMIENTO REPRESA\",\n",
    "        \"MANTENIMIENTO\",\n",
    "        \"TRANSFORMADORES\",\n",
    "        \"SEGURIDAD PRESA\",\n",
    "        \"PUERTO SECO\"\n",
    "    ],\n",
    "    \"Servicios Médicos y Sociales\": [\n",
    "        \"PRESTAMOS PERSONAL\",\n",
    "        \"SERVICIOS MÉDICOS\",\n",
    "        \"ATENCIÓN MÉDICA\",\n",
    "        \"SEGUROS VIDA\"\n",
    "    ],\n",
    "    \"Comunicaciones y Relaciones Institucionales\": [\n",
    "        \"RELACIONES PÚBLICAS\",\n",
    "        \"COMUNICACIÓN INSTITUCIONAL\",\n",
    "        \"PUBLICIDAD\",\n",
    "        \"LOGOTIPO\",\n",
    "        \"FIRMA DIGITAL\",\n",
    "        \"PAGINA WEB\"\n",
    "    ],\n",
    "    \"Auditorías y Control Interno\": [\n",
    "        \"AUDITORIA INTERNA\",\n",
    "        \"AUDITORÍA EXTERNA\",\n",
    "        \"CONTROL GESTIÓN\"\n",
    "    ],\n",
    "    \"Recursos Humanos\": [\n",
    "        \"CONTROL ASISTENCIA\",\n",
    "        \"EPIDEMIOLOGÍA\",\n",
    "        \"CURSOS Y SEMINARIOS\",\n",
    "        \"DESVINCULACIONES\"\n",
    "    ],\n",
    "    \"Ambiental y Social\": [\n",
    "        \"RIBEREÑOS\",\n",
    "        \"MERCOSUR\",\n",
    "        \"MERCADOS ELÉCTRICOS\",\n",
    "        \"CULTURALES\",\n",
    "        \"SAOTTA\"\n",
    "    ],\n",
    "    \"Gestión y Operaciones\": [\n",
    "        \"DIRECCIÓN EJECUTIVA\",\n",
    "        \"GERENCIA\",\n",
    "        \"DELEGACIONES\",\n",
    "        \"TAISG\",\n",
    "        \"HORARIOS OFICINAS\",\n",
    "        \"GRUPOS TRABAJO\",\n",
    "        \"CENTRO DOCUMENTACIÓN\",\n",
    "        \"RENOVACIÓN SALTO GRANDE\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt adaptado\n",
    "classification_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"data_summary\", \"categorias\"],\n",
    "    template=(\n",
    "        \"Basándote en el siguiente texto y en los datos extraídos y resumen:\\n\\n\"\n",
    "        \"Texto: {text}\\n\\n\"\n",
    "        \"Datos y resumen: {data_summary}\\n\\n\"\n",
    "        \"Considerando estas categorías disponibles: {categorias}\\n\\n\"\n",
    "        \"Clasifica la resolución estrictamente dentro de una de estas categorías principales.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Función ajustada\n",
    "def classify_resolution(text, data_summary):\n",
    "    categorias_str = ', '.join(CATEGORIAS_CTMSG.keys())\n",
    "    chain = LLMChain(llm=llm, prompt=classification_prompt)\n",
    "    classification = chain.run(text=text, data_summary=data_summary, categorias=categorias_str)\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función principal para procesar el PDF y guardar la información en TinyDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_data_summary(data_summary):\n",
    "    \"\"\"\n",
    "    Divide el resumen extraído en campos separados.\n",
    "    \"\"\"\n",
    "    fields = [\"Número de resolución\", \"Fecha\", \"Ente emisor\", \"Resumen ejecutivo breve\"]\n",
    "    parsed_data = {}\n",
    "\n",
    "    for field in fields:\n",
    "        pattern = rf\"{field}:\\s*(.+)\"\n",
    "        match = re.search(pattern, data_summary)\n",
    "        if match:\n",
    "            parsed_data[field] = match.group(1).strip()\n",
    "        else:\n",
    "            parsed_data[field] = None\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def process_pdf(pdf_path, extraction_template, db_table):\n",
    "    \"\"\"\n",
    "    Procesa un archivo PDF completo: extrae el texto, lo corrige,\n",
    "    extrae datos y resumen, clasifica la resolución y guarda todo en TinyDB.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    \n",
    "    # Verificar si el documento ya se encuentra en la base de datos\n",
    "    Record = Query()\n",
    "    if db_table.search(Record.pdf_file == file_name):\n",
    "        print(f\"El documento {file_name} ya se encuentra registrado en TinyDB.\")\n",
    "        return\n",
    "\n",
    "    # Extracción de texto\n",
    "    raw_text = pdf_to_text(pdf_path)\n",
    "    print(f\"Texto extraído del archivo {file_name}.\")\n",
    "    \n",
    "    # Corrección de la transcripción\n",
    "    corrected_text = correct_transcription(raw_text)\n",
    "    print(\"Transcripción corregida.\")\n",
    "    \n",
    "    # Extracción de datos y resumen\n",
    "    extracted_result = extract_data_summary(corrected_text, extraction_template)\n",
    "    print(\"Datos y resumen extraídos.\")\n",
    "    \n",
    "    # Clasificación de la resolución\n",
    "    classification = classify_resolution(corrected_text, extracted_result)\n",
    "    print(\"Clasificación realizada.\")\n",
    "    \n",
    "    # Crear el registro con la información resultante\n",
    "    parsed_summary = parse_data_summary(extracted_result)\n",
    "\n",
    "    record = {\n",
    "        \"pdf_file\": file_name,\n",
    "        \"text\": corrected_text,\n",
    "        \"numero_resolucion\": parsed_summary[\"Número de resolución\"],\n",
    "        \"fecha\": parsed_summary[\"Fecha\"],\n",
    "        \"ente_emisor\": parsed_summary[\"Ente emisor\"],\n",
    "        \"resumen_ejecutivo\": parsed_summary[\"Resumen ejecutivo breve\"],\n",
    "        \"classification\": classification\n",
    "    }\n",
    "    \n",
    "    # Guardar el registro en TinyDB\n",
    "    db_table.insert(record)\n",
    "    print(f\"El documento {file_name} ha sido registrado en TinyDB.\")\n",
    "\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución del procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento res00222.pdf ya se encuentra registrado en TinyDB.\n",
      "El documento res00122.pdf ya se encuentra registrado en TinyDB.\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# Template explícito para extracción\n",
    "extraction_template = (\n",
    "    \"- Número de resolución:\\n\"\n",
    "    \"- Fecha:\\n\"\n",
    "    \"- Ente emisor:\\n\"\n",
    "    \"- Resumen ejecutivo breve (máximo 30 palabras, tema principal solamente):\"\n",
    ")\n",
    "\n",
    "# Carpeta donde se encuentran los archivos PDF\n",
    "pdf_folder = \"pdfs\"  # Asegúrate de tener esta carpeta y colocar allí tus PDFs\n",
    "\n",
    "# Procesar cada PDF nuevo en la carpeta\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, file_name)\n",
    "        process_pdf(pdf_path, extraction_template, records_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función para procesar un nuevo PDF individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto extraído del archivo res00422.pdf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/98ml0k7s50sbjln7zqxcj2y80000gn/T/ipykernel_10972/3386155856.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=correction_prompt)\n",
      "/var/folders/66/98ml0k7s50sbjln7zqxcj2y80000gn/T/ipykernel_10972/3386155856.py:12: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  corrected = chain.run(text=text)\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "def procesar_nuevo_pdf(pdf_path, extraction_template, db_table):\n",
    "    \"\"\"\n",
    "    Procesa un nuevo PDF individualmente y lo añade a la base de datos TinyDB.\n",
    "    \"\"\"\n",
    "        # Crear el registro con la información resultante\n",
    "    record = {\n",
    "        \"pdf_file\": \"New Resolucion\",\n",
    "        \"text\": \"corrected_text\",\n",
    "        \"data_summary\": \"extracted_result\",\n",
    "        \"classification\": \"classification\"\n",
    "    }\n",
    "\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    Record = Query()\n",
    "    if db_table.search(Record.pdf_file == file_name):\n",
    "        print(f\"El documento {file_name} ya se encuentra registrado.\")\n",
    "        return\n",
    "    else:\n",
    "        record = process_pdf(pdf_path, extraction_template, db_table)\n",
    "        print(record)\n",
    "        \n",
    "\n",
    "#Ejemplo de llamada para procesar un nuevo PDF:\n",
    "nuevo_pdf = \"pdfs/new/res00422.pdf\"\n",
    "procesar_nuevo_pdf(nuevo_pdf, extraction_template, records_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspeccion de DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_file:\n",
      "res00222.pdf\n",
      "--------------------------------------------------\n",
      "text:\n",
      "COMISIÓN TÉCNICA MIXTA DE SALTO GRANDE  \n",
      "Videoconferencia, 26 de enero de 2022  \n",
      "RESOLUCIÓN CTM Nº 002/22 (Acta Nº 1127 - as. 8.2) \n",
      " \n",
      "VISTO: el EE 2021-SCYC-0144 en el que tramita la Resolución CTM Nº 173/21 de fecha 15.09.21, que deja sin efecto la Resolución CTM Nº 140/21 y aprueba el Pliego de Ba...\n",
      "--------------------------------------------------\n",
      "numero_resolucion:\n",
      "CTM Nº 002/22\n",
      "--------------------------------------------------\n",
      "fecha:\n",
      "26 de enero de 2022\n",
      "--------------------------------------------------\n",
      "ente_emisor:\n",
      "Comisión Técnica Mixta de Salto Grande\n",
      "--------------------------------------------------\n",
      "resumen_ejecutivo:\n",
      "Se declara fracasada la Licitación Pública del Contrato SG-723 “Reconversión de monte forestal en ambientes nativos”.\n",
      "--------------------------------------------------\n",
      "classification:\n",
      "La resolución CTM Nº 002/22, que declara fracasada la Licitación Pública del Contrato SG-723 “Reconversión de monte forestal en ambientes nativos”, se clasifica estrictamente dentro de la categoría principal de \"Contrataciones y Convenios\"....\n",
      "--------------------------------------------------\n",
      "{'pdf_file': 'res00222.pdf', 'text': 'COMISIÓN TÉCNICA MIXTA DE SALTO GRANDE  \\nVideoconferencia, 26 de enero de 2022  \\nRESOLUCIÓN CTM Nº 002/22 (Acta Nº 1127 - as. 8.2) \\n \\nVISTO: el EE 2021-SCYC-0144 en el que tramita la Resolución CTM Nº 173/21 de fecha 15.09.21, que deja sin efecto la Resolución CTM Nº 140/21 y aprueba el Pliego de Bases y Condiciones de la Licitación Pública del Contrato SG-723 “Reconversión de monte forestal en ambientes nativos” y dispone su llamado; y  \\nCONSIDERANDO: que, el 10.12.21 se efectuó el acto de apertura de ofertas, habiéndose presentado las firmas: Bidegain Servicios y Etchevers Francisco;  \\nque, la Asesoría Letrada en la actuación 18 considera admisibles las ofertas recibidas, pero observa que ambas superan ampliamente el monto pre-valorizado, por lo que recomienda declarar fracasada la referida Licitación;  \\nque, en la actuación 21 el Área Gestión Ambiental eleva el informe técnico de las ofertas, considerando admisible únicamente la de Bidegain Servicios;  \\nque, la Gerencia de Ingeniería y Planeamiento en la actuación 22 acompaña la recomendación de la Asesoría Letrada en cuanto al fracaso del proceso;  \\nque, la Gerencia General en la actuación 25, tomando en cuenta lo informado en las actuaciones 23 y 24, y la propuesta de la Asesoría Letrada eleva a consideración declarar fracasada la Licitación Pública del Contrato SG-723;  \\nlo establecido en el artículo 5.1. del Manual de Normas 003 - Compras y Contrataciones; y  \\nlo deliberado en Sala;  \\nLA COMISIÓN TÉCNICA MIXTA DE SALTO GRANDE  \\nR E S U E L V E:  \\n1. DECLARAR FRACASADA la Licitación Pública del Contrato SG-723 “Reconversión de monte forestal en ambientes nativos”, dispuesta por la Resolución CTM Nº 173/21 de fecha 15.09.21.  \\n2. ENCOMENDAR a la Gerencia General las acciones necesarias para el cumplimiento de lo resuelto, como así también las notificaciones correspondientes, así como las acciones necesarias para el cumplimiento del objeto de la licitación fracasada a través de los procedimientos que correspondan.  \\n3. COMUNICAR la presente Resolución a las Delegaciones Argentina y del Uruguay, a la Gerencia General, a la Asesoría Letrada, a las Auditorías Generales y al Polo Educativo, Científico, Tecnológico y Productivo de Salto Grande.  \\n4. PASE, a sus efectos, a la Secretaría General.  \\n \\nNicolás Irigoyen  \\nProsecretario  \\n\\nLuis Benedetto  \\nPresidente', 'numero_resolucion': 'CTM Nº 002/22', 'fecha': '26 de enero de 2022', 'ente_emisor': 'Comisión Técnica Mixta de Salto Grande', 'resumen_ejecutivo': 'Se declara fracasada la Licitación Pública del Contrato SG-723 “Reconversión de monte forestal en ambientes nativos”.', 'classification': 'La resolución CTM Nº 002/22, que declara fracasada la Licitación Pública del Contrato SG-723 “Reconversión de monte forestal en ambientes nativos”, se clasifica estrictamente dentro de la categoría principal de \"Contrataciones y Convenios\".'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db = TinyDB(\"ResCTM.json\")\n",
    "records_table = db.table(\"records\")\n",
    "\n",
    "registro_4 = records_table.get(doc_id=2)\n",
    "if registro_4:\n",
    "    for campo, valor in registro_4.items():\n",
    "        print(f\"{campo}:\")\n",
    "        if isinstance(valor, str) and len(valor) > 150:\n",
    "            print(valor[:300] + \"...\")\n",
    "        else:\n",
    "            print(valor)\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"Registro no encontrado.\")\n",
    "\n",
    "print(registro_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = TinyDB('historial.json')\n",
    "#db.truncate()  # Esto vacía completamente la base de datos\n",
    "#db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
